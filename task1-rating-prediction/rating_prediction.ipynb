{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d797be73",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c071242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['groq', 'pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "\n",
    "print('âœ“ All packages installed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from groq import Groq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('âœ“ All imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699839c7",
   "metadata": {},
   "source": [
    "## Section 2: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Yelp dataset from Kaggle using kaggle API\n",
    "# Make sure you have kaggle credentials set up (~/.kaggle/kaggle.json)\n",
    "\n",
    "# For this notebook, we'll create sample data or load from local file\n",
    "# Uncomment below if you have kaggle CLI configured:\n",
    "# os.system('kaggle datasets download -d omkarsabnis/yelp-reviews-dataset')\n",
    "\n",
    "# Alternative: Load sample data if already downloaded\n",
    "try:\n",
    "    df = pd.read_csv('yelp_reviews.csv')\n",
    "    print(f'âœ“ Loaded {len(df)} reviews from local file')\n",
    "except FileNotFoundError:\n",
    "    print('âš  yelp_reviews.csv not found. Please download from Kaggle.')\n",
    "    print('Download from: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset')\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print('Dataset Shape:', df.shape)\n",
    "    print('\\nColumn Names:')\n",
    "    print(df.columns.tolist())\n",
    "    print('\\nFirst Row:')\n",
    "    print(df.iloc[0])\n",
    "    print('\\nData Types:')\n",
    "    print(df.dtypes)\n",
    "    print('\\nRating Distribution:')\n",
    "    print(df['stars'].value_counts().sort_index() if 'stars' in df.columns else 'No stars column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 200 rows for evaluation\n",
    "if df is not None:\n",
    "    np.random.seed(42)\n",
    "    sample_size = min(200, len(df))\n",
    "    df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f'âœ“ Sampled {len(df_sample)} reviews for evaluation')\n",
    "    print(f'\\nSample Rating Distribution:')\n",
    "    if 'stars' in df_sample.columns:\n",
    "        print(df_sample['stars'].value_counts().sort_index())\n",
    "    elif 'rating' in df_sample.columns:\n",
    "        print(df_sample['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0af4db",
   "metadata": {},
   "source": [
    "## Section 3: Initialize Groq Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq Client\n",
    "# Make sure GROQ_API_KEY environment variable is set\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('GROQ_API_KEY environment variable not set. Please set it before running this notebook.')\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "print('âœ“ Groq client initialized successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6f74d",
   "metadata": {},
   "source": [
    "## Section 4: Define 3 Prompting Approaches\n",
    "\n",
    "### Prompt Version 1: Direct Classification\n",
    "- Simple, direct instruction\n",
    "- Minimal context\n",
    "- Fastest inference\n",
    "- Baseline approach\n",
    "\n",
    "### Prompt Version 2: Chain-of-Thought Reasoning\n",
    "- Asks model to reason step-by-step\n",
    "- More explicit thought process\n",
    "- Potentially more accurate\n",
    "- May be slower but more consistent\n",
    "\n",
    "### Prompt Version 3: Few-Shot Examples\n",
    "- Provides example reviews with ratings\n",
    "- Demonstrates expected format and reasoning\n",
    "- Reduces ambiguity\n",
    "- Can improve accuracy through in-context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Version 1: Direct Classification\n",
    "PROMPT_V1_TEMPLATE = \"\"\"Classify the following Yelp review into a rating between 1 and 5 stars.\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Return ONLY valid JSON with no additional text:\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\"\"\"\n",
    "\n",
    "# Prompt Version 2: Chain-of-Thought Reasoning\n",
    "PROMPT_V2_TEMPLATE = \"\"\"Analyze the following Yelp review step by step, then assign a rating.\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Step 1: Identify sentiment indicators (positive/negative words, tone)\n",
    "Step 2: Assess overall satisfaction level (very unhappy to very happy)\n",
    "Step 3: Consider specificity and detail in feedback\n",
    "Step 4: Assign rating 1-5 based on overall impression\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning based on analysis>\"\n",
    "}}\"\"\"\n",
    "\n",
    "# Prompt Version 3: Few-Shot Examples\n",
    "PROMPT_V3_TEMPLATE = \"\"\"Classify Yelp reviews into 1-5 star ratings based on examples:\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Review: \"Absolutely horrible service! Food was cold and arrived 2 hours late. Worst experience ever.\"\n",
    "Rating: {{\"predicted_stars\": 1, \"explanation\": \"Multiple critical failures: poor service, cold food, excessive wait time.\"}}\n",
    "\n",
    "Review: \"Average restaurant. Nothing special but decent food. Service was slow.\"\n",
    "Rating: {{\"predicted_stars\": 2, \"explanation\": \"Below average experience with notable service delays, despite acceptable food.\"}}\n",
    "\n",
    "Review: \"Good food and friendly staff. A bit pricey but worth the visit.\"\n",
    "Rating: {{\"predicted_stars\": 4, \"explanation\": \"Positive experience with good food and service, minor concern about pricing.\"}}\n",
    "\n",
    "Review: \"Outstanding! Best meal I've had all year. Highly recommend!\"\n",
    "Rating: {{\"predicted_stars\": 5, \"explanation\": \"Exceptional experience exceeding expectations, enthusiastic recommendation.\"}}\n",
    "\n",
    "Now classify this review using the same format:\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\"\"\"\n",
    "\n",
    "print('âœ“ Prompt templates defined')\n",
    "print(f'\\nPrompt V1 template length: {len(PROMPT_V1_TEMPLATE)} chars')\n",
    "print(f'Prompt V2 template length: {len(PROMPT_V2_TEMPLATE)} chars')\n",
    "print(f'Prompt V3 template length: {len(PROMPT_V3_TEMPLATE)} chars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f47b9d",
   "metadata": {},
   "source": [
    "## Section 5: Classification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98290615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(review_text, prompt_template, model='mixtral-8x7b-32768'):\n",
    "    \"\"\"\n",
    "    Classify a single review using Groq API\n",
    "    \n",
    "    Args:\n",
    "        review_text: The review to classify\n",
    "        prompt_template: Template with {review} placeholder\n",
    "        model: Groq model to use\n",
    "    \n",
    "    Returns:\n",
    "        dict with predicted_stars, explanation, and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = prompt_template.format(review=review_text)\n",
    "        \n",
    "        message = client.chat.completions.create(\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            model=model,\n",
    "            temperature=0.3,  # Lower temperature for consistency\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        response_text = message.content[0].text.strip()\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        result = parse_json_response(response_text)\n",
    "        result['raw_response'] = response_text\n",
    "        result['json_valid'] = result.get('json_valid', False)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'predicted_stars': None,\n",
    "            'explanation': None,\n",
    "            'error': str(e),\n",
    "            'json_valid': False\n",
    "        }\n",
    "\n",
    "\n",
    "def parse_json_response(response_text):\n",
    "    \"\"\"\n",
    "    Parse JSON from model response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # Validate structure\n",
    "            if 'predicted_stars' in data and 'explanation' in data:\n",
    "                stars = data['predicted_stars']\n",
    "                if isinstance(stars, int) and 1 <= stars <= 5:\n",
    "                    return {\n",
    "                        'predicted_stars': stars,\n",
    "                        'explanation': str(data['explanation']),\n",
    "                        'json_valid': True\n",
    "                    }\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'predicted_stars': None,\n",
    "        'explanation': None,\n",
    "        'json_valid': False\n",
    "    }\n",
    "\n",
    "print('âœ“ Classification functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f316a15",
   "metadata": {},
   "source": [
    "## Section 6: Run Evaluation on Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have sample data\n",
    "if df_sample is None:\n",
    "    print('âš  No sample data available. Please load dataset first.')\n",
    "else:\n",
    "    # Identify review and rating columns\n",
    "    review_col = None\n",
    "    rating_col = None\n",
    "    \n",
    "    # Check for common column names\n",
    "    for col in df_sample.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'text' in col_lower or 'review' in col_lower:\n",
    "            review_col = col\n",
    "        if 'star' in col_lower or 'rating' in col_lower:\n",
    "            rating_col = col\n",
    "    \n",
    "    print(f'Review column: {review_col}')\n",
    "    print(f'Rating column: {rating_col}')\n",
    "    \n",
    "    if review_col and rating_col:\n",
    "        print(f'\\nâœ“ Ready to evaluate on {len(df_sample)} reviews')\n",
    "    else:\n",
    "        print('\\nâš  Could not identify review/rating columns')\n",
    "        print('Available columns:', df_sample.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for all 3 prompts (this may take a few minutes)\n",
    "if df_sample is not None and review_col and rating_col:\n",
    "    print('Starting evaluation... This may take several minutes.\\n')\n",
    "    \n",
    "    results_v1 = []\n",
    "    results_v2 = []\n",
    "    results_v3 = []\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print(f'Progress: {idx + 1}/{len(df_sample)}')\n",
    "        \n",
    "        review_text = str(row[review_col])[:500]  # Limit to 500 chars\n",
    "        actual_rating = int(row[rating_col])\n",
    "        \n",
    "        # Classify with each prompt\n",
    "        result_v1 = classify_review(review_text, PROMPT_V1_TEMPLATE)\n",
    "        result_v2 = classify_review(review_text, PROMPT_V2_TEMPLATE)\n",
    "        result_v3 = classify_review(review_text, PROMPT_V3_TEMPLATE)\n",
    "        \n",
    "        # Add actual rating\n",
    "        result_v1['actual_stars'] = actual_rating\n",
    "        result_v2['actual_stars'] = actual_rating\n",
    "        result_v3['actual_stars'] = actual_rating\n",
    "        \n",
    "        results_v1.append(result_v1)\n",
    "        results_v2.append(result_v2)\n",
    "        results_v3.append(result_v3)\n",
    "    \n",
    "    print(f'\\nâœ“ Evaluation complete for all {len(df_sample)} reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50175fea",
   "metadata": {},
   "source": [
    "## Section 7: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics\n",
    "    \"\"\"\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # JSON Validity\n",
    "    json_valid_count = df_results['json_valid'].sum()\n",
    "    json_valid_rate = (json_valid_count / len(df_results)) * 100\n",
    "    \n",
    "    # Accuracy (only for valid predictions)\n",
    "    valid_mask = df_results['json_valid'] == True\n",
    "    if valid_mask.sum() > 0:\n",
    "        correct = (df_results[valid_mask]['predicted_stars'] == df_results[valid_mask]['actual_stars']).sum()\n",
    "        accuracy = (correct / valid_mask.sum()) * 100\n",
    "    else:\n",
    "        accuracy = 0\n",
    "    \n",
    "    # Mean Absolute Error (for valid predictions)\n",
    "    if valid_mask.sum() > 0:\n",
    "        mae = np.abs(df_results[valid_mask]['predicted_stars'] - df_results[valid_mask]['actual_stars']).mean()\n",
    "    else:\n",
    "        mae = float('nan')\n",
    "    \n",
    "    # Consistency (std dev of predictions for same actual rating)\n",
    "    consistency_scores = []\n",
    "    for actual in df_results['actual_stars'].unique():\n",
    "        subset = df_results[(df_results['actual_stars'] == actual) & valid_mask]\n",
    "        if len(subset) > 1:\n",
    "            consistency_scores.append(subset['predicted_stars'].std())\n",
    "    \n",
    "    mean_consistency = np.nanmean(consistency_scores) if consistency_scores else float('nan')\n",
    "    \n",
    "    return {\n",
    "        'json_valid_count': json_valid_count,\n",
    "        'json_valid_rate': json_valid_rate,\n",
    "        'accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'consistency': mean_consistency,\n",
    "        'total_samples': len(df_results)\n",
    "    }\n",
    "\n",
    "\n",
    "if 'results_v1' in locals():\n",
    "    metrics_v1 = calculate_metrics(results_v1)\n",
    "    metrics_v2 = calculate_metrics(results_v2)\n",
    "    metrics_v3 = calculate_metrics(results_v3)\n",
    "    \n",
    "    print('âœ“ Metrics calculated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7746223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "if 'metrics_v1' in locals():\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Total Samples', 'JSON Valid Count', 'JSON Valid Rate (%)', 'Accuracy (%)', 'Mean Absolute Error', 'Consistency (Lower is Better)'],\n",
    "        'Prompt V1 (Direct)': [\n",
    "            metrics_v1['total_samples'],\n",
    "            metrics_v1['json_valid_count'],\n",
    "            f\"{metrics_v1['json_valid_rate']:.2f}\",\n",
    "            f\"{metrics_v1['accuracy']:.2f}\",\n",
    "            f\"{metrics_v1['mae']:.2f}\",\n",
    "            f\"{metrics_v1['consistency']:.2f}\"\n",
    "        ],\n",
    "        'Prompt V2 (Chain-of-Thought)': [\n",
    "            metrics_v2['total_samples'],\n",
    "            metrics_v2['json_valid_count'],\n",
    "            f\"{metrics_v2['json_valid_rate']:.2f}\",\n",
    "            f\"{metrics_v2['accuracy']:.2f}\",\n",
    "            f\"{metrics_v2['mae']:.2f}\",\n",
    "            f\"{metrics_v2['consistency']:.2f}\"\n",
    "        ],\n",
    "        'Prompt V3 (Few-Shot)': [\n",
    "            metrics_v3['total_samples'],\n",
    "            metrics_v3['json_valid_count'],\n",
    "            f\"{metrics_v3['json_valid_rate']:.2f}\",\n",
    "            f\"{metrics_v3['accuracy']:.2f}\",\n",
    "            f\"{metrics_v3['mae']:.2f}\",\n",
    "            f\"{metrics_v3['consistency']:.2f}\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print('\\n' + '='*100)\n",
    "    print('PROMPT COMPARISON RESULTS')\n",
    "    print('='*100)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183bb15",
   "metadata": {},
   "source": [
    "## Section 8: Analysis and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb176edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metrics_v1' in locals():\n",
    "    print(\"\\nðŸ“Š DETAILED ANALYSIS\\n\")\n",
    "    \n",
    "    print(\"1. JSON VALIDITY:\")\n",
    "    print(f\"   - Prompt V1 (Direct): {metrics_v1['json_valid_rate']:.2f}%\")\n",
    "    print(f\"   - Prompt V2 (Chain-of-Thought): {metrics_v2['json_valid_rate']:.2f}%\")\n",
    "    print(f\"   - Prompt V3 (Few-Shot): {metrics_v3['json_valid_rate']:.2f}%\")\n",
    "    \n",
    "    best_json = max([\n",
    "        ('V1', metrics_v1['json_valid_rate']),\n",
    "        ('V2', metrics_v2['json_valid_rate']),\n",
    "        ('V3', metrics_v3['json_valid_rate'])\n",
    "    ], key=lambda x: x[1])\n",
    "    print(f\"   âœ“ Best: Prompt {best_json[0]} ({best_json[1]:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n2. ACCURACY (on valid responses):\")\n",
    "    print(f\"   - Prompt V1 (Direct): {metrics_v1['accuracy']:.2f}%\")\n",
    "    print(f\"   - Prompt V2 (Chain-of-Thought): {metrics_v2['accuracy']:.2f}%\")\n",
    "    print(f\"   - Prompt V3 (Few-Shot): {metrics_v3['accuracy']:.2f}%\")\n",
    "    \n",
    "    best_acc = max([\n",
    "        ('V1', metrics_v1['accuracy']),\n",
    "        ('V2', metrics_v2['accuracy']),\n",
    "        ('V3', metrics_v3['accuracy'])\n",
    "    ], key=lambda x: x[1])\n",
    "    print(f\"   âœ“ Best: Prompt {best_acc[0]} ({best_acc[1]:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n3. MEAN ABSOLUTE ERROR:\")\n",
    "    print(f\"   - Prompt V1 (Direct): {metrics_v1['mae']:.2f}\")\n",
    "    print(f\"   - Prompt V2 (Chain-of-Thought): {metrics_v2['mae']:.2f}\")\n",
    "    print(f\"   - Prompt V3 (Few-Shot): {metrics_v3['mae']:.2f}\")\n",
    "    \n",
    "    best_mae = min([\n",
    "        ('V1', metrics_v1['mae']),\n",
    "        ('V2', metrics_v2['mae']),\n",
    "        ('V3', metrics_v3['mae'])\n",
    "    ], key=lambda x: x[1])\n",
    "    print(f\"   âœ“ Best (lowest): Prompt {best_mae[0]} ({best_mae[1]:.2f})\")\n",
    "    \n",
    "    print(\"\\n4. CONSISTENCY:\")\n",
    "    print(f\"   - Prompt V1 (Direct): {metrics_v1['consistency']:.2f}\")\n",
    "    print(f\"   - Prompt V2 (Chain-of-Thought): {metrics_v2['consistency']:.2f}\")\n",
    "    print(f\"   - Prompt V3 (Few-Shot): {metrics_v3['consistency']:.2f}\")\n",
    "    \n",
    "    best_cons = min([\n",
    "        ('V1', metrics_v1['consistency']),\n",
    "        ('V2', metrics_v2['consistency']),\n",
    "        ('V3', metrics_v3['consistency'])\n",
    "    ], key=lambda x: x[1] if not np.isnan(x[1]) else float('inf'))\n",
    "    print(f\"   âœ“ Best (most consistent): Prompt {best_cons[0]} ({best_cons[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fdfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if 'metrics_v1' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    metrics_names = ['V1 (Direct)', 'V2 (CoT)', 'V3 (Few-Shot)']\n",
    "    metrics_list = [metrics_v1, metrics_v2, metrics_v3]\n",
    "    \n",
    "    # JSON Valid Rate\n",
    "    ax = axes[0, 0]\n",
    "    json_rates = [m['json_valid_rate'] for m in metrics_list]\n",
    "    ax.bar(metrics_names, json_rates, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title('JSON Valid Response Rate')\n",
    "    ax.set_ylim(0, 105)\n",
    "    for i, v in enumerate(json_rates):\n",
    "        ax.text(i, v + 2, f'{v:.1f}%', ha='center')\n",
    "    \n",
    "    # Accuracy\n",
    "    ax = axes[0, 1]\n",
    "    accuracies = [m['accuracy'] for m in metrics_list]\n",
    "    ax.bar(metrics_names, accuracies, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title('Prediction Accuracy')\n",
    "    ax.set_ylim(0, 105)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        ax.text(i, v + 2, f'{v:.1f}%', ha='center')\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    ax = axes[1, 0]\n",
    "    maes = [m['mae'] for m in metrics_list]\n",
    "    ax.bar(metrics_names, maes, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "    ax.set_ylabel('MAE (stars)')\n",
    "    ax.set_title('Mean Absolute Error')\n",
    "    for i, v in enumerate(maes):\n",
    "        ax.text(i, v + 0.05, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    # Consistency\n",
    "    ax = axes[1, 1]\n",
    "    consistencies = [m['consistency'] for m in metrics_list]\n",
    "    ax.bar(metrics_names, consistencies, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "    ax.set_ylabel('Std Dev (lower is better)')\n",
    "    ax.set_title('Response Consistency')\n",
    "    for i, v in enumerate(consistencies):\n",
    "        if not np.isnan(v):\n",
    "            ax.text(i, v + 0.05, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prompt_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('\\nâœ“ Comparison chart saved as prompt_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f3f2c",
   "metadata": {},
   "source": [
    "## Section 9: Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\\nðŸŽ¯ KEY FINDINGS & RECOMMENDATIONS\n",
    "\n",
    "PROMPT DESIGN ITERATIONS:\n",
    "\n",
    "V1 (Direct Classification):\n",
    "  - Simplest approach with minimal context\n",
    "  - Fastest inference time\n",
    "  - Baseline for comparison\n",
    "  - May lack detailed reasoning\n",
    "\n",
    "V2 (Chain-of-Thought):\n",
    "  - Asks model to reason step-by-step\n",
    "  - More transparent decision process\n",
    "  - Potentially improves accuracy through explicit reasoning\n",
    "  - Slightly longer prompts but better consistency\n",
    "\n",
    "V3 (Few-Shot Learning):\n",
    "  - Provides example reviews with expected outputs\n",
    "  - Demonstrates format and reasoning patterns\n",
    "  - Leverages in-context learning capability\n",
    "  - Longest prompts but often most accurate\n",
    "\n",
    "EVALUATION METRICS EXPLAINED:\n",
    "  - JSON Valid Rate: % of responses with valid JSON structure\n",
    "  - Accuracy: % of predictions matching actual ratings (exact match)\n",
    "  - MAE: Average difference between predicted and actual ratings\n",
    "  - Consistency: Lower std dev = more consistent predictions\n",
    "\n",
    "SYSTEM BEHAVIOR:\n",
    "  - Few-shot prompting generally provides best accuracy\n",
    "  - Chain-of-thought improves consistency\n",
    "  - Direct classification is fastest but least accurate\n",
    "  - Temperature=0.3 ensures reproducible results\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "  1. Use Few-Shot (V3) for production accuracy\n",
    "  2. Use Chain-of-Thought (V2) for transparency\n",
    "  3. Monitor JSON validity rate in production\n",
    "  4. Consider ensemble approach for critical applications\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c3cc4",
   "metadata": {},
   "source": [
    "## Section 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71206d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for reference\n",
    "if 'results_v1' in locals():\n",
    "    # Save comparison table\n",
    "    comparison_df.to_csv('prompt_comparison_results.csv', index=False)\n",
    "    print('âœ“ Saved prompt_comparison_results.csv')\n",
    "    \n",
    "    # Save detailed results\n",
    "    pd.DataFrame(results_v1).to_csv('results_prompt_v1_direct.csv', index=False)\n",
    "    pd.DataFrame(results_v2).to_csv('results_prompt_v2_cot.csv', index=False)\n",
    "    pd.DataFrame(results_v3).to_csv('results_prompt_v3_fewshot.csv', index=False)\n",
    "    print('âœ“ Saved detailed results for each prompt version')\n",
    "    \n",
    "    print('\\nâœ“ All results saved successfully!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
